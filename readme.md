# [Hierarchical Attentin Networds Experiment](http://www.aclweb.org/anthology/N16-1174)

Code is based on:
 [ilivans/tf-rnn-attention](https://github.com/ilivans/tf-rnn-attention)

Reference also and recommended:
 [Effective Approaches to Attention-based Neural Machine Translation](http://aclweb.org/anthology/D15-1166) 

## Basic Enviornment and Requirement:
- Python 3.6
- Tensorflow-gpu 1.4.0
- dataset keras.datasets.imdb (about 17 Mb)

### Train self-model:
```
python run.py <--train> 
```

### Test Models:
```
python run.py --test
```

